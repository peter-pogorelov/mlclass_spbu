{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Релизация модели FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_score(y_pred, y_real):\n",
    "    return np.sqrt(np.sum(np.power(y_pred - y_real, 2)) / y_pred.shape[0])\n",
    "\n",
    "def R2_score(y_pred, y_real):\n",
    "    return 1 - np.sum(np.power(y_pred - y_real, 2)) / np.sum(np.power(y_real - np.mean(y_real), 2))\n",
    "\n",
    "def R2_adj_score(y_pred, y_real, features):\n",
    "    R2 = R2_score(y_pred, y_real)\n",
    "    return 1 - (1 - R2) * (y_real.shape[0] - 1) / (y_real.shape[0] - features - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD2WAYFactorizationMachine(object):\n",
    "    def __init__(self, \n",
    "                 k=5, \n",
    "                 epochs=5, \n",
    "                 batch_size = 64, \n",
    "                 epsilon=1e-4, \n",
    "                 step=0.001, \n",
    "                 step_V=None, \n",
    "                 lr_decay_rate=5e-7,\n",
    "                 verbose=0, \n",
    "                 nobs_verbose=10\n",
    "        ):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.step = step\n",
    "        self.verbose = verbose\n",
    "        self.nobs_verbose = nobs_verbose\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        if step_V:\n",
    "            self.stepV = step_V\n",
    "        else:\n",
    "            self.stepV = self.step / 10\n",
    "        self.k = k\n",
    "        \n",
    "        self.w0 = None\n",
    "        self.w1 = None\n",
    "        self.V = None\n",
    "        \n",
    "        self.Z = None\n",
    "        self.Z_squared = None\n",
    "    \n",
    "    def exp_decay(self, initial_lrate, epoch):\n",
    "        k = self.lr_decay_rate\n",
    "        lrate = initial_lrate * np.exp(-k*epoch)\n",
    "        return lrate\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.w0 = 0\n",
    "        self.w1 = np.random.normal(size=(X.shape[1], 1), scale=.1)\n",
    "        self.V = np.random.normal(size=(X.shape[1], self.k), scale=.1)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        timedelta = time.time()\n",
    "        for epoch in range(2, self.epochs + 2):\n",
    "            if self.verbose and epoch % self.nobs_verbose == 0:\n",
    "                print(\"\\r Progress: {}/{} ({}%), duration: {} sec\".format(\n",
    "                    epoch, self.epochs + 1, int(((epoch+1) / (self.epochs)) * 100), int(time.time() - timedelta)\n",
    "                ), end=\"\")\n",
    "                timedelta = time.time()\n",
    "            \n",
    "            batch_indices = np.random.choice(n, self.batch_size)\n",
    "            \n",
    "            # densify sparse matrices\n",
    "            x = X[batch_indices, :]\n",
    "            if hasattr(X, 'todense'):\n",
    "                x = np.asarray(x.todense())\n",
    "            y = Y[batch_indices]\n",
    "            \n",
    "            y_hat = self.predict(x)\n",
    "            dy = -2 * (y - y_hat)\n",
    "            \n",
    "            self.w0 -= np.mean(dy) * self.step\n",
    "            self.w1 -= np.mean(dy * x, axis=0, keepdims=True).T * self.exp_decay(self.step, epoch)\n",
    "            \n",
    "            for f in range(self.k):\n",
    "                left = x * self.Z[:, f].reshape(-1, 1)\n",
    "                right = self.X_squared * self.V[:, f]\n",
    "                self.V[:, f] -=  np.mean(dy * (left - right), axis=0) * self.exp_decay(self.stepV, epoch)\n",
    "    \n",
    "    def calculate_nonlinear_part(self, X):\n",
    "        self.Z = X @ self.V\n",
    "        self.X_squared = np.power(X, 2)\n",
    "        return 1/2 * np.sum(\n",
    "            np.power(self.Z, 2) - self.X_squared @ np.power(self.V, 2), \n",
    "            axis=1, keepdims=True\n",
    "        )\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.w0 + X @ self.w1 + self.calculate_nonlinear_part(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка корректности работы алгоритма на примере искусственного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# и сравнение с SGD регрессором sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = make_regression(n_samples=10000, n_features=4, n_targets=4, n_informative=2, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X, y[:, :-1]], axis=1)\n",
    "y = y[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = SGD2WAYFactorizationMachine(epochs=20000, batch_size=256, k=8, step=1e-5, step_V=1e-8, verbose=1, nobs_verbose=5000, lr_decay_rate=1e-5)\n",
    "reg = SGDRegressor(fit_intercept=True, penalty='none', shuffle=False, learning_rate='constant', eta0=0.001, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.001,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "       loss='squared_loss', max_iter=10000, n_iter=None, penalty='none',\n",
       "       power_t=0.25, random_state=None, shuffle=False, tol=None, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X, y.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress: 20000/20001 (100%), duration: 3 sec"
     ]
    }
   ],
   "source": [
    "fm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.65034225e+08,  1.74924163e+10, -1.91707160e+11, -3.20168751e+08,\n",
       "        -4.60966195e+10, -4.30955970e+10, -3.49519559e+10]),\n",
       " array([5.38982709e+10]))"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0658891 ],\n",
       "        [ 0.09245032],\n",
       "        [ 0.06325088],\n",
       "        [ 0.06153187],\n",
       "        [ 0.08707379],\n",
       "        [-0.16187898],\n",
       "        [ 0.3424324 ]]),\n",
       " 0.04277680891446344,\n",
       " array([[ 0.05304461,  0.04999643, -0.01406342,  0.05861141, -0.15165712,\n",
       "          0.04917029,  0.09304436,  0.1058377 ],\n",
       "        [ 0.07627837,  0.1543007 , -0.00335402, -0.17078368, -0.09273973,\n",
       "         -0.02209483,  0.123709  ,  0.03692013],\n",
       "        [-0.05447968,  0.01259196, -0.26783772, -0.08084042,  0.06597526,\n",
       "         -0.0545381 , -0.10490917,  0.05618324],\n",
       "        [-0.05429117,  0.08523016,  0.21162872, -0.03963514, -0.04638106,\n",
       "         -0.00514878,  0.00390355,  0.10121871],\n",
       "        [-0.01963095, -0.00914506, -0.01507281, -0.00844009,  0.00246768,\n",
       "          0.00383982,  0.00084308, -0.02185324],\n",
       "        [ 0.00655918,  0.07292702, -0.02914311, -0.00246394,  0.04966344,\n",
       "          0.02354367,  0.02647275, -0.00898529],\n",
       "        [-0.01503185,  0.01698133,  0.04201216,  0.00802643,  0.02300353,\n",
       "         -0.02847201, -0.04278446, -0.04620225]]))"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.w1, fm.w0, fm.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предсказания полученные SGD регрессией\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.28540890e+13,  9.19552143e+12,  1.23298966e+13,  2.18284721e+13,\n",
       "        2.42433683e+13])"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('предсказания полученные SGD регрессией')\n",
    "reg.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предсказания полученные FM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -4.49029517],\n",
       "       [-48.66375385],\n",
       "       [-29.58515453],\n",
       "       [-44.13095484],\n",
       "       [-19.54679126]])"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('предсказания полученные FM')\n",
    "fm.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "реальные значения таргета\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -4.54589026],\n",
       "       [-51.17486706],\n",
       "       [-30.37219423],\n",
       "       [-44.65509058],\n",
       "       [-19.45926933]])"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('реальные значения таргета')\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc):\n",
    "    # movies ids\n",
    "    film_ids = list(map(lambda x: int(x[:-1]), re.findall(r'\\d+\\:', doc)))\n",
    "    # separate frames\n",
    "    frames_raw = re.split(r'\\d+\\:', doc)\n",
    "    frames_raw = frames_raw[1:]\n",
    "    \n",
    "    frames_totale = []\n",
    "\n",
    "    for frame, movie_id in zip(frames_raw, film_ids):\n",
    "        sub_df = pd.read_csv(StringIO(frame), names=['CustomerID','Rating','Date'])\n",
    "        sub_df['MovieID'] = movie_id\n",
    "\n",
    "        frames_totale.append(sub_df)\n",
    "\n",
    "    dataset = pd.concat(frames_totale)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txtfile:\n",
    "    del txtfile\n",
    "\n",
    "with open('Task2/netflix-prize-data/combined_data_1.txt', 'r') as f:\n",
    "    txtfile = f.read()\n",
    "    \n",
    "dataset_p1 = process_document(txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if txtfile:\n",
    "#    del txtfile\n",
    "#\n",
    "#with open('Task2/netflix-prize-data/combined_data_2.txt', 'r') as f:\n",
    "#    txtfile = f.read()\n",
    "#    \n",
    "#dataset_p2 = process_document(txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_p1\n",
    "dataset = dataset[dataset.MovieID.isin(movie_table.index.unique().values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.concat([dataset_p1, dataset_p2], axis=0)\n",
    "#dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Таблица с фичами для фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txtfile:\n",
    "    del txtfile\n",
    "    \n",
    "with open('Task2/netflix-prize-data/movie_titles.csv', 'r') as f:\n",
    "    txtfile = f.read()\n",
    "\n",
    "subs = re.compile(r'\\d+,\\d+,')\n",
    "new_lines = []\n",
    "for i in txtfile.split('\\n'):\n",
    "    sub_part = subs.sub('', i)\n",
    "    if 'NULL' not in sub_part:\n",
    "        if ',' not in sub_part:\n",
    "            new_lines.append(i)\n",
    "        else:\n",
    "            left_part = i.replace(sub_part, '')\n",
    "            right_part = sub_part\n",
    "            new_lines.append(left_part + '\"' + right_part + '\"')\n",
    "        \n",
    "movie_info = '\\n'.join(new_lines)\n",
    "movie_table = pd.read_csv(StringIO(movie_info), names = ['MovieID', 'Year', 'Name'])\n",
    "\n",
    "movie_table.MovieID = movie_table.MovieID.astype(int)\n",
    "movie_table.Year = movie_table.Year.astype(int)\n",
    "\n",
    "movie_table = movie_table.set_index('MovieID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем спарс матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_alias = {j:i for i, j in enumerate(dataset.CustomerID.unique())}\n",
    "movie_alias = {j:i for i, j in enumerate(dataset.MovieID.unique())}\n",
    "year_alias = {j:i for i, j in enumerate(movie_table.Year.unique())}\n",
    "\n",
    "\n",
    "f1_size = dataset.MovieID.unique().shape[0]\n",
    "f2_size = dataset.CustomerID.unique().shape[0]\n",
    "f3_size = movie_table.Year.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_dataset = csr_matrix((f1_size + f2_size + f3_size, dataset.size))\n",
    "sparse_dataset = lil_matrix((dataset.shape[0], f1_size + f2_size + f3_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress: 24050000/24053575 (99%)"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(dataset.itertuples()):\n",
    "    if j % 50000 == 0:\n",
    "        print(\"\\r Progress: {}/{} ({}%)\".format(j, dataset.shape[0], int(((j+1) / dataset.shape[0]) * 100)), end=\"\")\n",
    "    #print(i)\n",
    "    customer = customer_alias[i.CustomerID]\n",
    "    movie = movie_alias[i.MovieID]\n",
    "    year = year_alias[movie_table.loc[i.MovieID].Year]\n",
    "    \n",
    "    sparse_dataset[j, customer] = 1\n",
    "    sparse_dataset[j, f1_size + movie] = 1\n",
    "    sparse_dataset[j, f1_size + f2_size + year] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.Rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_sparse_dataset = csr_matrix(sparse_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sparse_df_1.bin']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(csr_sparse_dataset, 'sparse_df_1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM на полной матрице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      " Progress: 100/375838 (0%), duration: 515 sec"
     ]
    }
   ],
   "source": [
    "rmse_train = []\n",
    "rmse_test = []\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "r2_adj_train = []\n",
    "r2_adj_test = []\n",
    "\n",
    "kfold = KFold(n_splits=4)\n",
    "\n",
    "count = 1\n",
    "for ix_train, ix_test in kfold.split(csr_sparse_dataset):\n",
    "    print('{}-th fold'.format(count))\n",
    "    mdl = SGD2WAYFactorizationMachine(epochs=csr_sparse_dataset.shape[0] // 64, batch_size=128, k=2, step=.001, step_V=0.00001, verbose=True, nobs_verbose=100)\n",
    "    \n",
    "    X_train = csr_sparse_dataset[ix_train]\n",
    "    y_train = y[ix_train].reshape(-1, 1)\n",
    "    \n",
    "    X_test = csr_sparse_dataset[ix_test]\n",
    "    y_test = y[ix_test].reshape(-1, 1)\n",
    "    \n",
    "    mdl.fit(X_train, y_train)\n",
    "    \n",
    "    rmse_test.append(RMSE_score(mdl.predict(X_test), y_test))\n",
    "    r2_test.append(R2_score(mdl.predict(X_test), y_test))\n",
    "    r2_adj_test.append(R2_adj_score(mdl.predict(X_test), y_test, csr_sparse_dataset.shape[1]))\n",
    "    \n",
    "    rmse_train.append(RMSE_score(mdl.predict(X_train), y_train))\n",
    "    r2_train.append(R2_score(mdl.predict(X_train), y_train))\n",
    "    r2_adj_train.append(R2_adj_score(mdl.predict(X_train), y_train, csr_sparse_dataset.shape[1]))\n",
    "    \n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
