{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Релизация модели FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_score(y_pred, y_real):\n",
    "    return np.sqrt(np.sum(np.power(y_pred - y_real, 2)) / y_pred.shape[0])\n",
    "\n",
    "def R2_score(y_pred, y_real):\n",
    "    return 1 - np.sum(np.power(y_pred - y_real, 2)) / np.sum(np.power(y_real - np.mean(y_real), 2))\n",
    "\n",
    "def R2_adj_score(y_pred, y_real, features):\n",
    "    R2 = R2_score(y_pred, y_real)\n",
    "    return 1 - (1 - R2) * (y_real.shape[0] - 1) / (y_real.shape[0] - features - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD2WAYFactorizationMachine(object):\n",
    "    def __init__(self, \n",
    "                 k=5, \n",
    "                 epochs=5, \n",
    "                 batch_size = 64, \n",
    "                 epsilon=1e-4, \n",
    "                 step=0.001, \n",
    "                 step_V=None, \n",
    "                 lr_decay_rate=5e-7,\n",
    "                 verbose=0, \n",
    "                 nobs_verbose=10\n",
    "        ):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.step = step\n",
    "        self.verbose = verbose\n",
    "        self.nobs_verbose = nobs_verbose\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        if step_V:\n",
    "            self.stepV = step_V\n",
    "        else:\n",
    "            self.stepV = self.step / 10\n",
    "        self.k = k\n",
    "        \n",
    "        self.w0 = None\n",
    "        self.w1 = None\n",
    "        self.V = None\n",
    "        \n",
    "        self.Z = None\n",
    "        self.Z_squared = None\n",
    "    \n",
    "    def exp_decay(self, initial_lrate, epoch):\n",
    "        k = self.lr_decay_rate\n",
    "        lrate = initial_lrate * np.exp(-k*epoch)\n",
    "        return lrate\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # works only with sparse matrices\n",
    "        self.w0 = 0\n",
    "        self.w1 = np.random.normal(size=(X.shape[1], 1), scale=.1)\n",
    "        self.V = np.random.normal(size=(X.shape[1], self.k), scale=.1)\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        timedelta = time.time()\n",
    "        for epoch in range(2, self.epochs + 2):\n",
    "            if self.verbose and epoch % self.nobs_verbose == 0:\n",
    "                print(\"\\r Progress: {}/{} ({}%), duration: {} sec\".format(\n",
    "                    epoch, self.epochs + 1, int(((epoch+1) / (self.epochs)) * 100), int(time.time() - timedelta)\n",
    "                ), end=\"\")\n",
    "                timedelta = time.time()\n",
    "            \n",
    "            batch_indices = np.random.choice(n, self.batch_size)\n",
    "            \n",
    "            # densify sparse matrices\n",
    "            x = X[batch_indices, :]\n",
    "            y = Y[batch_indices]\n",
    "            y_hat = self.predict(x)\n",
    "            \n",
    "            dy = -2 * (y - y_hat)\n",
    "            \n",
    "            self.w0 -= np.mean(dy) * self.step\n",
    "            if hasattr(x, 'todense'):\n",
    "                self.w1 -= x.multiply(dy).mean(axis=0).T * self.exp_decay(self.step, epoch)\n",
    "            else:\n",
    "                self.w1 -= np.mean(x * dy, axis=0, keepdims=True).T * self.exp_decay(self.step, epoch)\n",
    "            \n",
    "            for f in range(self.k):\n",
    "                if hasattr(x, 'todense'):\n",
    "                    left = x.multiply(self.Z[:, f].reshape(-1, 1))\n",
    "                    right = self.X_squared.multiply(self.V[:, f])\n",
    "                    self.V[:, f] -= (np.asarray((left - right).multiply(dy).mean(axis=0).T) * self.exp_decay(self.stepV, epoch)).squeeze()\n",
    "                else:\n",
    "                    left = x * self.Z[:, f].reshape(-1, 1)\n",
    "                    right = self.X_squared * self.V[:, f]\n",
    "                    self.V[:, f] -=  np.mean(dy * (left - right), axis=0) * self.exp_decay(self.stepV, epoch)\n",
    "    \n",
    "    def calculate_nonlinear_part(self, X):\n",
    "        self.Z = X @ self.V\n",
    "        \n",
    "        if hasattr(X, 'todense'):\n",
    "            self.X_squared = X.power(2)#np.power(X, 2)\n",
    "        else:\n",
    "            self.X_squared = np.power(X, 2)\n",
    "        \n",
    "        return 1/2 * np.sum(\n",
    "            np.power(self.Z, 2) - self.X_squared @ np.power(self.V, 2), \n",
    "            axis=1, keepdims=True\n",
    "        )\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.w0 + X @ self.w1 + self.calculate_nonlinear_part(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка корректности работы алгоритма на примере искусственного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# и сравнение с SGD регрессором sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = make_regression(n_samples=10000, n_features=4, n_targets=4, n_informative=2, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X, y[:, :-1]], axis=1)\n",
    "y = y[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = SGD2WAYFactorizationMachine(epochs=20000, batch_size=256, k=8, step=1e-5, step_V=1e-8, verbose=1, nobs_verbose=5000, lr_decay_rate=1e-5)\n",
    "reg = SGDRegressor(fit_intercept=True, penalty='none', shuffle=False, learning_rate='constant', eta0=0.001, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ppogorelov/VirtualEnv/v0/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.001, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='squared_loss', max_iter=10000,\n",
       "       n_iter=None, n_iter_no_change=5, penalty='none', power_t=0.25,\n",
       "       random_state=None, shuffle=False, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X, y.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress: 20000/20001 (100%), duration: 3 sec"
     ]
    }
   ],
   "source": [
    "fm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.75329105e+09, -1.17467670e+09, -7.18091047e+10,  8.85153006e+09,\n",
       "         1.00692797e+11, -3.43837503e+10,  3.66537441e+10]),\n",
       " array([9.38799482e+10]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.10038559],\n",
       "        [-0.02807447],\n",
       "        [ 0.16951733],\n",
       "        [-0.02117882],\n",
       "        [-0.28724782],\n",
       "        [ 0.2635045 ],\n",
       "        [ 0.93081021]]),\n",
       " 0.053373035151704586,\n",
       " array([[-0.05867992,  0.1301452 ,  0.1888644 ,  0.00655247,  0.05789429,\n",
       "         -0.05814951, -0.11663405,  0.00558362],\n",
       "        [ 0.09327705,  0.11236647, -0.07478194, -0.18236786, -0.07602433,\n",
       "         -0.15691578, -0.02714165,  0.0772217 ],\n",
       "        [-0.14407354,  0.19513548,  0.06879782,  0.03924446,  0.04392671,\n",
       "          0.01467825, -0.02790113, -0.03484998],\n",
       "        [-0.14528486, -0.05072692, -0.04303462, -0.05974964, -0.01978388,\n",
       "          0.02746084, -0.16375715, -0.18189178],\n",
       "        [-0.09042885, -0.1907012 , -0.09081672,  0.14776575,  0.09549876,\n",
       "          0.02505465, -0.00708806,  0.11401723],\n",
       "        [ 0.06120799, -0.00096996, -0.11455083,  0.01317796, -0.07844233,\n",
       "          0.17829282,  0.18398215, -0.01070418],\n",
       "        [ 0.11176101,  0.06013751, -0.11215329,  0.07775149,  0.13295085,\n",
       "         -0.00854505, -0.05249132, -0.10999891]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.w1, fm.w0, fm.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предсказания полученные SGD регрессией\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.66206158e+12,  1.75817059e+13, -3.38099890e+10,  4.49951384e+12,\n",
       "        3.39950471e+12])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('предсказания полученные SGD регрессией')\n",
    "reg.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "предсказания полученные FM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-21.24971036],\n",
       "       [309.61409397],\n",
       "       [ 93.40545585],\n",
       "       [ 91.35701438],\n",
       "       [-18.08437594]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('предсказания полученные FM')\n",
    "fm.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "реальные значения таргета\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-20.72177077],\n",
       "       [311.99005337],\n",
       "       [ 93.37714163],\n",
       "       [ 90.39896741],\n",
       "       [-18.37828621]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('реальные значения таргета')\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc):\n",
    "    # movies ids\n",
    "    film_ids = list(map(lambda x: int(x[:-1]), re.findall(r'\\d+\\:', doc)))\n",
    "    # separate frames\n",
    "    frames_raw = re.split(r'\\d+\\:', doc)\n",
    "    frames_raw = frames_raw[1:]\n",
    "    \n",
    "    frames_totale = []\n",
    "\n",
    "    for frame, movie_id in zip(frames_raw, film_ids):\n",
    "        sub_df = pd.read_csv(StringIO(frame), names=['CustomerID','Rating','Date'])\n",
    "        sub_df['MovieID'] = movie_id\n",
    "\n",
    "        frames_totale.append(sub_df)\n",
    "\n",
    "    dataset = pd.concat(frames_totale)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txtfile:\n",
    "    del txtfile\n",
    "\n",
    "with open('Task2/netflix-prize-data/combined_data_1.txt', 'r') as f:\n",
    "    txtfile = f.read()\n",
    "    \n",
    "dataset_p1 = process_document(txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if txtfile:\n",
    "#    del txtfile\n",
    "#\n",
    "#with open('Task2/netflix-prize-data/combined_data_2.txt', 'r') as f:\n",
    "#    txtfile = f.read()\n",
    "#    \n",
    "#dataset_p2 = process_document(txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_p1\n",
    "dataset = dataset[dataset.MovieID.isin(movie_table.index.unique().values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.concat([dataset_p1, dataset_p2], axis=0)\n",
    "#dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Таблица с фичами для фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfile = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if txtfile:\n",
    "    del txtfile\n",
    "    \n",
    "with open('Task2/netflix-prize-data/movie_titles.csv', 'r') as f:\n",
    "    txtfile = f.read()\n",
    "\n",
    "subs = re.compile(r'\\d+,\\d+,')\n",
    "new_lines = []\n",
    "for i in txtfile.split('\\n'):\n",
    "    sub_part = subs.sub('', i)\n",
    "    if 'NULL' not in sub_part:\n",
    "        if ',' not in sub_part:\n",
    "            new_lines.append(i)\n",
    "        else:\n",
    "            left_part = i.replace(sub_part, '')\n",
    "            right_part = sub_part\n",
    "            new_lines.append(left_part + '\"' + right_part + '\"')\n",
    "        \n",
    "movie_info = '\\n'.join(new_lines)\n",
    "movie_table = pd.read_csv(StringIO(movie_info), names = ['MovieID', 'Year', 'Name'])\n",
    "\n",
    "movie_table.MovieID = movie_table.MovieID.astype(int)\n",
    "movie_table.Year = movie_table.Year.astype(int)\n",
    "\n",
    "movie_table = movie_table.set_index('MovieID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем спарс матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_alias = {j:i for i, j in enumerate(dataset.CustomerID.unique())}\n",
    "movie_alias = {j:i for i, j in enumerate(dataset.MovieID.unique())}\n",
    "year_alias = {j:i for i, j in enumerate(movie_table.Year.unique())}\n",
    "\n",
    "\n",
    "f1_size = dataset.MovieID.unique().shape[0]\n",
    "f2_size = dataset.CustomerID.unique().shape[0]\n",
    "f3_size = movie_table.Year.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_dataset = csr_matrix((f1_size + f2_size + f3_size, dataset.size))\n",
    "sparse_dataset = lil_matrix((dataset.shape[0], f1_size + f2_size + f3_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Progress: 24050000/24053575 (99%)"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(dataset.itertuples()):\n",
    "    if j % 50000 == 0:\n",
    "        print(\"\\r Progress: {}/{} ({}%)\".format(j, dataset.shape[0], int(((j+1) / dataset.shape[0]) * 100)), end=\"\")\n",
    "    #print(i)\n",
    "    customer = customer_alias[i.CustomerID]\n",
    "    movie = movie_alias[i.MovieID]\n",
    "    year = year_alias[movie_table.loc[i.MovieID].Year]\n",
    "    \n",
    "    sparse_dataset[j, customer] = 1\n",
    "    sparse_dataset[j, f1_size + movie] = 1\n",
    "    sparse_dataset[j, f1_size + f2_size + year] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.Rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_sparse_dataset = csr_matrix(sparse_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sparse_df_1.bin']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(csr_sparse_dataset, 'sparse_df_1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_sparse_dataset = joblib.load('sparse_df_1.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FM на полной матрице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      " Progress: 187900/187919 (99%), duration: 3 sec2-th fold\n",
      " Progress: 187900/187919 (99%), duration: 3 sec3-th fold\n",
      " Progress: 187900/187919 (99%), duration: 3 sec4-th fold\n",
      " Progress: 187900/187919 (99%), duration: 4 sec"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4559eac3f969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mrmse_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRMSE_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mr2_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mr2_adj_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR2_adj_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr_sparse_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-276bf5ad27d2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_nonlinear_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-276bf5ad27d2>\u001b[0m in \u001b[0;36mcalculate_nonlinear_part\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         return 1/2 * np.sum(\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_squared\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         )\n",
      "\u001b[0;32m~/VirtualEnv/v0/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rsub_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtualEnv/v0/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/VirtualEnv/v0/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m            \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m            indptr, indices, data)\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rmse_train = []\n",
    "rmse_test = []\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "r2_adj_train = []\n",
    "r2_adj_test = []\n",
    "\n",
    "kfold = KFold(n_splits=4)\n",
    "\n",
    "count = 1\n",
    "for ix_train, ix_test in kfold.split(csr_sparse_dataset):\n",
    "    print('{}-th fold'.format(count))\n",
    "    mdl = SGD2WAYFactorizationMachine(epochs=csr_sparse_dataset.shape[0] // 128, batch_size=256, k=2, step=.001, step_V=0.00001, verbose=True, nobs_verbose=100)\n",
    "    \n",
    "    X_train = csr_sparse_dataset[ix_train]\n",
    "    y_train = y[ix_train].reshape(-1, 1)\n",
    "    \n",
    "    X_test = csr_sparse_dataset[ix_test]\n",
    "    y_test = y[ix_test].reshape(-1, 1)\n",
    "    \n",
    "    mdl.fit(X_train, y_train)\n",
    "    \n",
    "    rmse_test.append(RMSE_score(mdl.predict(X_test), y_test))\n",
    "    r2_test.append(R2_score(mdl.predict(X_test), y_test))\n",
    "    r2_adj_test.append(R2_adj_score(mdl.predict(X_test), y_test, csr_sparse_dataset.shape[1]))\n",
    "    \n",
    "    rmse_train.append(RMSE_score(mdl.predict(X_train), y_train))\n",
    "    r2_train.append(R2_score(mdl.predict(X_train), y_train))\n",
    "    r2_adj_train.append(R2_adj_score(mdl.predict(X_train), y_train, csr_sparse_dataset.shape[1]))\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0870387847803527,\n",
       " 1.0912097795611546,\n",
       " 1.1055237437197836,\n",
       " 1.1061612819509088]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0478473775797363, 1.0490578400791624, 1.048496854418923, 1.0475487576807785]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse_test</th>\n",
       "      <td>1.087039</td>\n",
       "      <td>1.091210</td>\n",
       "      <td>1.105524</td>\n",
       "      <td>1.106161</td>\n",
       "      <td>1.097483</td>\n",
       "      <td>0.009805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse_train</th>\n",
       "      <td>1.047847</td>\n",
       "      <td>1.049058</td>\n",
       "      <td>1.048497</td>\n",
       "      <td>1.047549</td>\n",
       "      <td>1.048238</td>\n",
       "      <td>0.000675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3      mean       std\n",
       "rmse_test   1.087039  1.091210  1.105524  1.106161  1.097483  0.009805\n",
       "rmse_train  1.047847  1.049058  1.048497  1.047549  1.048238  0.000675"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.vstack([rmse_test, rmse_train]), index=[\n",
    "    'rmse_test',\n",
    "    'rmse_train',\n",
    "])\n",
    "\n",
    "df = pd.concat([df, df.mean(axis=1).rename('mean'), df.std(axis=1).rename('std')], axis=1)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
